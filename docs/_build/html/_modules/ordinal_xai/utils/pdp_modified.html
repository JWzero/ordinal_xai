<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ordinal_xai.utils.pdp_modified &#8212; ordinal_xai  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=27fed22d" />
    <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for ordinal_xai.utils.pdp_modified</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Partial dependence plots for regression and classification models.</span>

<span class="sd">Modification of the existing scikit-learn pdp module thats is compatible with ordinal regression estimators.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: The scikit-learn developers</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Iterable</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">sparse</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats.mstats</span><span class="w"> </span><span class="kn">import</span> <span class="n">mquantiles</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_classifier</span><span class="p">,</span> <span class="n">is_regressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble._gb</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseGradientBoosting</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble._hist_gradient_boosting.gradient_boosting</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">BaseHistGradientBoosting</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Bunch</span><span class="p">,</span> <span class="n">_safe_indexing</span><span class="p">,</span> <span class="n">check_array</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils._indexing</span><span class="w"> </span><span class="kn">import</span> <span class="n">_determine_key_type</span><span class="p">,</span> <span class="n">_get_column_indices</span><span class="p">,</span> <span class="n">_safe_assign</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils._optional_dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">check_matplotlib_support</span>  <span class="c1"># noqa</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils._param_validation</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">HasMethods</span><span class="p">,</span>
    <span class="n">Integral</span><span class="p">,</span>
    <span class="n">Interval</span><span class="p">,</span>
    <span class="n">StrOptions</span><span class="p">,</span>
    <span class="n">validate_params</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">._response_modified</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_response_values</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils.extmath</span><span class="w"> </span><span class="kn">import</span> <span class="n">cartesian</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils.validation</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_sample_weight</span><span class="p">,</span> <span class="n">check_is_fitted</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.inspection._pd_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_feature_names</span><span class="p">,</span> <span class="n">_get_feature_index</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;partial_dependence&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_grid_from_X</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">percentiles</span><span class="p">,</span> <span class="n">is_categorical</span><span class="p">,</span> <span class="n">grid_resolution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate a grid of points based on the percentiles of X.</span>

<span class="sd">    The grid is a cartesian product between the columns of ``values``. The</span>
<span class="sd">    ith column of ``values`` consists in ``grid_resolution`` equally-spaced</span>
<span class="sd">    points between the percentiles of the jth column of X.</span>

<span class="sd">    If ``grid_resolution`` is bigger than the number of unique values in the</span>
<span class="sd">    j-th column of X or if the feature is a categorical feature (by inspecting</span>
<span class="sd">    `is_categorical`) , then those unique values will be used instead.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_target_features)</span>
<span class="sd">        The data.</span>

<span class="sd">    percentiles : tuple of float</span>
<span class="sd">        The percentiles which are used to construct the extreme values of</span>
<span class="sd">        the grid. Must be in [0, 1].</span>

<span class="sd">    is_categorical : list of bool</span>
<span class="sd">        For each feature, tells whether it is categorical or not. If a feature</span>
<span class="sd">        is categorical, then the values used will be the unique ones</span>
<span class="sd">        (i.e. categories) instead of the percentiles.</span>

<span class="sd">    grid_resolution : int</span>
<span class="sd">        The number of equally spaced points to be placed on the grid for each</span>
<span class="sd">        feature.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grid : ndarray of shape (n_points, n_target_features)</span>
<span class="sd">        A value for each feature at each point in the grid. ``n_points`` is</span>
<span class="sd">        always ``&lt;= grid_resolution ** X.shape[1]``.</span>

<span class="sd">    values : list of 1d ndarrays</span>
<span class="sd">        The values with which the grid has been created. The size of each</span>
<span class="sd">        array ``values[j]`` is either ``grid_resolution``, or the number of</span>
<span class="sd">        unique values in ``X[:, j]``, whichever is smaller.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">percentiles</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">percentiles</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;percentiles&#39; must be a sequence of 2 elements.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">percentiles</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;percentiles&#39; values must be in [0, 1].&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">percentiles</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">percentiles</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;percentiles[0] must be strictly less than percentiles[1].&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">grid_resolution</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;grid_resolution&#39; must be strictly greater than 1.&quot;</span><span class="p">)</span>

    <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># TODO: we should handle missing values (i.e. `np.nan`) specifically and store them</span>
    <span class="c1"># in a different Bunch attribute.</span>
    <span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">is_cat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">is_categorical</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">uniques</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
            <span class="c1"># `np.unique` will fail in the presence of `np.nan` and `str` categories</span>
            <span class="c1"># due to sorting. Temporary, we reraise an error explaining the problem.</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The column #</span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s2"> contains mixed data types. Finding unique &quot;</span>
                <span class="s2">&quot;categories fail due to sorting. It usually means that the column &quot;</span>
                <span class="s2">&quot;contains `np.nan` values together with `str` categories. Such use &quot;</span>
                <span class="s2">&quot;case is not yet supported in scikit-learn.&quot;</span>
            <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">exc</span>
        <span class="k">if</span> <span class="n">is_cat</span> <span class="ow">or</span> <span class="n">uniques</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">grid_resolution</span><span class="p">:</span>
            <span class="c1"># Use the unique values either because:</span>
            <span class="c1"># - feature has low resolution use unique values</span>
            <span class="c1"># - feature is categorical</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="n">uniques</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># create axis based on percentiles and grid resolution</span>
            <span class="n">emp_percentiles</span> <span class="o">=</span> <span class="n">mquantiles</span><span class="p">(</span>
                <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">prob</span><span class="o">=</span><span class="n">percentiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">emp_percentiles</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">emp_percentiles</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;percentiles are too close to each other, &quot;</span>
                    <span class="s2">&quot;unable to build the grid. Please choose percentiles &quot;</span>
                    <span class="s2">&quot;that are further apart.&quot;</span>
                <span class="p">)</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                <span class="n">emp_percentiles</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">emp_percentiles</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">num</span><span class="o">=</span><span class="n">grid_resolution</span><span class="p">,</span>
                <span class="n">endpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cartesian</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">values</span>

<span class="k">def</span><span class="w"> </span><span class="nf">_partial_dependence_brute</span><span class="p">(</span>
    <span class="n">est</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">response_method</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate partial dependence via the brute force method.</span>

<span class="sd">    The brute method explicitly averages the predictions of an estimator over a</span>
<span class="sd">    grid of feature values.</span>

<span class="sd">    For each `grid` value, all the samples from `X` have their variables of</span>
<span class="sd">    interest replaced by that specific `grid` value. The predictions are then made</span>
<span class="sd">    and averaged across the samples.</span>

<span class="sd">    This method is slower than the `&#39;recursion&#39;`</span>
<span class="sd">    (:func:`~sklearn.inspection._partial_dependence._partial_dependence_recursion`)</span>
<span class="sd">    version for estimators with this second option. However, with the `&#39;brute&#39;`</span>
<span class="sd">    force method, the average will be done with the given `X` and not the `X`</span>
<span class="sd">    used during training, as it is done in the `&#39;recursion&#39;` version. Therefore</span>
<span class="sd">    the average can always accept `sample_weight` (even when the estimator was</span>
<span class="sd">    fitted without).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    est : BaseEstimator</span>
<span class="sd">        A fitted estimator object implementing :term:`predict`,</span>
<span class="sd">        :term:`predict_proba`, or :term:`decision_function`.</span>
<span class="sd">        Multioutput-multiclass classifiers are not supported.</span>

<span class="sd">    grid : array-like of shape (n_points, n_target_features)</span>
<span class="sd">        The grid of feature values for which the partial dependence is calculated.</span>
<span class="sd">        Note that `n_points` is the number of points in the grid and `n_target_features`</span>
<span class="sd">        is the number of features you are doing partial dependence at.</span>

<span class="sd">    features : array-like of {int, str}</span>
<span class="sd">        The feature (e.g. `[0]`) or pair of interacting features</span>
<span class="sd">        (e.g. `[(0, 1)]`) for which the partial dependency should be computed.</span>

<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        `X` is used to generate values for the complement features. That is, for</span>
<span class="sd">        each value in `grid`, the method will average the prediction of each</span>
<span class="sd">        sample from `X` having that grid value for `features`.</span>

<span class="sd">    response_method : {&#39;auto&#39;, &#39;predict_proba&#39;, &#39;decision_function&#39;}, \</span>
<span class="sd">            default=&#39;auto&#39;</span>
<span class="sd">        Specifies whether to use :term:`predict_proba` or</span>
<span class="sd">        :term:`decision_function` as the target response. For regressors</span>
<span class="sd">        this parameter is ignored and the response is always the output of</span>
<span class="sd">        :term:`predict`. By default, :term:`predict_proba` is tried first</span>
<span class="sd">        and we revert to :term:`decision_function` if it doesn&#39;t exist.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Sample weights are used to calculate weighted means when averaging the</span>
<span class="sd">        model output. If `None`, then samples are equally weighted. Note that</span>
<span class="sd">        `sample_weight` does not change the individual predictions.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    averaged_predictions : array-like of shape (n_targets, n_points)</span>
<span class="sd">        The averaged predictions for the given `grid` of features values.</span>
<span class="sd">        Note that `n_targets` is the number of targets (e.g. 1 for binary</span>
<span class="sd">        classification, `n_tasks` for multi-output regression, and `n_classes` for</span>
<span class="sd">        multiclass classification) and `n_points` is the number of points in the `grid`.</span>

<span class="sd">    predictions : array-like</span>
<span class="sd">        The predictions for the given `grid` of features values over the samples</span>
<span class="sd">        from `X`. For non-multioutput regression and binary classification the</span>
<span class="sd">        shape is `(n_instances, n_points)` and for multi-output regression and</span>
<span class="sd">        multiclass classification the shape is `(n_targets, n_instances, n_points)`,</span>
<span class="sd">        where `n_targets` is the number of targets (`n_tasks` for multi-output</span>
<span class="sd">        regression, and `n_classes` for multiclass classification), `n_instances`</span>
<span class="sd">        is the number of instances in `X`, and `n_points` is the number of points</span>
<span class="sd">        in the `grid`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">averaged_predictions</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">X_eval</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">new_values</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">variable</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
            <span class="n">_safe_assign</span><span class="p">(</span><span class="n">X_eval</span><span class="p">,</span> <span class="n">new_values</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">column_indexer</span><span class="o">=</span><span class="n">variable</span><span class="p">)</span>

        <span class="c1"># Note: predictions is of shape</span>
        <span class="c1"># (n_points,) for non-multioutput regressors</span>
        <span class="c1"># (n_points, n_tasks) for multioutput regressors</span>
        <span class="c1"># (n_points, 1) for the regressors in cross_decomposition (I think)</span>
        <span class="c1"># (n_points, 2) for binary classification</span>
        <span class="c1"># (n_points, n_classes) for multiclass classification</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_get_response_values</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">X_eval</span><span class="p">,</span> <span class="n">response_method</span><span class="o">=</span><span class="n">response_method</span><span class="p">)</span>

        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
        <span class="c1"># average over samples</span>
        <span class="n">averaged_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">))</span>

    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># reshape to (n_targets, n_instances, n_points) where n_targets is:</span>
    <span class="c1"># - 1 for non-multioutput regression and binary classification (shape is</span>
    <span class="c1">#   already correct in those cases)</span>
    <span class="c1"># - n_tasks for multi-output regression</span>
    <span class="c1"># - n_classes for multiclass classification.</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="k">if</span> <span class="n">is_regressor</span><span class="p">(</span><span class="n">est</span><span class="p">)</span> <span class="ow">and</span> <span class="n">predictions</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># non-multioutput regression, shape is (n_instances, n_points,)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">is_classifier</span><span class="p">(</span><span class="n">est</span><span class="p">)</span> <span class="ow">and</span> <span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Binary classification, shape is (2, n_instances, n_points).</span>
        <span class="c1"># we output the effect of **positive** class</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># reshape averaged_predictions to (n_targets, n_points) where n_targets is:</span>
    <span class="c1"># - 1 for non-multioutput regression and binary classification (shape is</span>
    <span class="c1">#   already correct in those cases)</span>
    <span class="c1"># - n_tasks for multi-output regression</span>
    <span class="c1"># - n_classes for multiclass classification.</span>
    <span class="n">averaged_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">averaged_predictions</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="k">if</span> <span class="n">is_regressor</span><span class="p">(</span><span class="n">est</span><span class="p">)</span> <span class="ow">and</span> <span class="n">averaged_predictions</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># non-multioutput regression, shape is (n_points,)</span>
        <span class="n">averaged_predictions</span> <span class="o">=</span> <span class="n">averaged_predictions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">is_classifier</span><span class="p">(</span><span class="n">est</span><span class="p">)</span> <span class="ow">and</span> <span class="n">averaged_predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Binary classification, shape is (2, n_points).</span>
        <span class="c1"># we output the effect of **positive** class</span>
        <span class="n">averaged_predictions</span> <span class="o">=</span> <span class="n">averaged_predictions</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">averaged_predictions</span> <span class="o">=</span> <span class="n">averaged_predictions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">averaged_predictions</span><span class="p">,</span> <span class="n">predictions</span>


<div class="viewcode-block" id="partial_dependence">
<a class="viewcode-back" href="../../../source/api/utils/pdp_modified.html#ordinal_xai.utils.pdp_modified.partial_dependence">[docs]</a>
<span class="nd">@validate_params</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;estimator&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">HasMethods</span><span class="p">([</span><span class="s2">&quot;fit&quot;</span><span class="p">,</span> <span class="s2">&quot;predict&quot;</span><span class="p">]),</span>
            <span class="n">HasMethods</span><span class="p">([</span><span class="s2">&quot;fit&quot;</span><span class="p">,</span> <span class="s2">&quot;predict_proba&quot;</span><span class="p">]),</span>
            <span class="n">HasMethods</span><span class="p">([</span><span class="s2">&quot;fit&quot;</span><span class="p">,</span> <span class="s2">&quot;decision_function&quot;</span><span class="p">]),</span>
        <span class="p">],</span>
        <span class="s2">&quot;X&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;array-like&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse matrix&quot;</span><span class="p">],</span>
        <span class="s2">&quot;features&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;array-like&quot;</span><span class="p">,</span> <span class="n">Integral</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="s2">&quot;sample_weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;array-like&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="s2">&quot;categorical_features&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;array-like&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="s2">&quot;feature_names&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;array-like&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="s2">&quot;response_method&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;predict_proba&quot;</span><span class="p">,</span> <span class="s2">&quot;decision_function&quot;</span><span class="p">})],</span>
        <span class="s2">&quot;percentiles&quot;</span><span class="p">:</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span>
        <span class="s2">&quot;grid_resolution&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;recursion&quot;</span><span class="p">,</span> <span class="s2">&quot;brute&quot;</span><span class="p">})],</span>
        <span class="s2">&quot;kind&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;average&quot;</span><span class="p">,</span> <span class="s2">&quot;individual&quot;</span><span class="p">,</span> <span class="s2">&quot;both&quot;</span><span class="p">})],</span>
    <span class="p">},</span>
    <span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">partial_dependence</span><span class="p">(</span>
    <span class="n">estimator</span><span class="p">,</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">features</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">categorical_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">percentiles</span><span class="o">=</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">),</span>
    <span class="n">grid_resolution</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;average&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Partial dependence of ``features``.</span>

<span class="sd">    Partial dependence of a feature (or a set of features) corresponds to</span>
<span class="sd">    the average response of an estimator for each possible value of the</span>
<span class="sd">    feature.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;partial_dependence&gt;`.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        For :class:`~sklearn.ensemble.GradientBoostingClassifier` and</span>
<span class="sd">        :class:`~sklearn.ensemble.GradientBoostingRegressor`, the</span>
<span class="sd">        `&#39;recursion&#39;` method (used by default) will not account for the `init`</span>
<span class="sd">        predictor of the boosting process. In practice, this will produce</span>
<span class="sd">        the same values as `&#39;brute&#39;` up to a constant offset in the target</span>
<span class="sd">        response, provided that `init` is a constant estimator (which is the</span>
<span class="sd">        default). However, if `init` is not a constant estimator, the</span>
<span class="sd">        partial dependence values are incorrect for `&#39;recursion&#39;` because the</span>
<span class="sd">        offset will be sample-dependent. It is preferable to use the `&#39;brute&#39;`</span>
<span class="sd">        method. Note that this only applies to</span>
<span class="sd">        :class:`~sklearn.ensemble.GradientBoostingClassifier` and</span>
<span class="sd">        :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to</span>
<span class="sd">        :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and</span>
<span class="sd">        :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : BaseEstimator</span>
<span class="sd">        A fitted estimator object implementing :term:`predict`,</span>
<span class="sd">        :term:`predict_proba`, or :term:`decision_function`.</span>
<span class="sd">        Multioutput-multiclass classifiers are not supported.</span>

<span class="sd">    X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)</span>
<span class="sd">        ``X`` is used to generate a grid of values for the target</span>
<span class="sd">        ``features`` (where the partial dependence will be evaluated), and</span>
<span class="sd">        also to generate values for the complement features when the</span>
<span class="sd">        `method` is &#39;brute&#39;.</span>

<span class="sd">    features : array-like of {int, str, bool} or int or str</span>
<span class="sd">        The feature (e.g. `[0]`) or pair of interacting features</span>
<span class="sd">        (e.g. `[(0, 1)]`) for which the partial dependency should be computed.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Sample weights are used to calculate weighted means when averaging the</span>
<span class="sd">        model output. If `None`, then samples are equally weighted. If</span>
<span class="sd">        `sample_weight` is not `None`, then `method` will be set to `&#39;brute&#39;`.</span>
<span class="sd">        Note that `sample_weight` is ignored for `kind=&#39;individual&#39;`.</span>

<span class="sd">        .. versionadded:: 1.3</span>

<span class="sd">    categorical_features : array-like of shape (n_features,) or shape \</span>
<span class="sd">            (n_categorical_features,), dtype={bool, int, str}, default=None</span>
<span class="sd">        Indicates the categorical features.</span>

<span class="sd">        - `None`: no feature will be considered categorical;</span>
<span class="sd">        - boolean array-like: boolean mask of shape `(n_features,)`</span>
<span class="sd">            indicating which features are categorical. Thus, this array has</span>
<span class="sd">            the same shape has `X.shape[1]`;</span>
<span class="sd">        - integer or string array-like: integer indices or strings</span>
<span class="sd">            indicating categorical features.</span>

<span class="sd">        .. versionadded:: 1.2</span>

<span class="sd">    feature_names : array-like of shape (n_features,), dtype=str, default=None</span>
<span class="sd">        Name of each feature; `feature_names[i]` holds the name of the feature</span>
<span class="sd">        with index `i`.</span>
<span class="sd">        By default, the name of the feature corresponds to their numerical</span>
<span class="sd">        index for NumPy array and their column name for pandas dataframe.</span>

<span class="sd">        .. versionadded:: 1.2</span>

<span class="sd">    response_method : {&#39;auto&#39;, &#39;predict_proba&#39;, &#39;decision_function&#39;}, \</span>
<span class="sd">            default=&#39;auto&#39;</span>
<span class="sd">        Specifies whether to use :term:`predict_proba` or</span>
<span class="sd">        :term:`decision_function` as the target response. For regressors</span>
<span class="sd">        this parameter is ignored and the response is always the output of</span>
<span class="sd">        :term:`predict`. By default, :term:`predict_proba` is tried first</span>
<span class="sd">        and we revert to :term:`decision_function` if it doesn&#39;t exist. If</span>
<span class="sd">        ``method`` is &#39;recursion&#39;, the response is always the output of</span>
<span class="sd">        :term:`decision_function`.</span>

<span class="sd">    percentiles : tuple of float, default=(0.05, 0.95)</span>
<span class="sd">        The lower and upper percentile used to create the extreme values</span>
<span class="sd">        for the grid. Must be in [0, 1].</span>

<span class="sd">    grid_resolution : int, default=100</span>
<span class="sd">        The number of equally spaced points on the grid, for each target</span>
<span class="sd">        feature.</span>

<span class="sd">    method : {&#39;auto&#39;, &#39;recursion&#39;, &#39;brute&#39;}, default=&#39;auto&#39;</span>
<span class="sd">        The method used to calculate the averaged predictions:</span>

<span class="sd">        - `&#39;recursion&#39;` is only supported for some tree-based estimators</span>
<span class="sd">          (namely</span>
<span class="sd">          :class:`~sklearn.ensemble.GradientBoostingClassifier`,</span>
<span class="sd">          :class:`~sklearn.ensemble.GradientBoostingRegressor`,</span>
<span class="sd">          :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,</span>
<span class="sd">          :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,</span>
<span class="sd">          :class:`~sklearn.tree.DecisionTreeRegressor`,</span>
<span class="sd">          :class:`~sklearn.ensemble.RandomForestRegressor`,</span>
<span class="sd">          ) when `kind=&#39;average&#39;`.</span>
<span class="sd">          This is more efficient in terms of speed.</span>
<span class="sd">          With this method, the target response of a</span>
<span class="sd">          classifier is always the decision function, not the predicted</span>
<span class="sd">          probabilities. Since the `&#39;recursion&#39;` method implicitly computes</span>
<span class="sd">          the average of the Individual Conditional Expectation (ICE) by</span>
<span class="sd">          design, it is not compatible with ICE and thus `kind` must be</span>
<span class="sd">          `&#39;average&#39;`.</span>

<span class="sd">        - `&#39;brute&#39;` is supported for any estimator, but is more</span>
<span class="sd">          computationally intensive.</span>

<span class="sd">        - `&#39;auto&#39;`: the `&#39;recursion&#39;` is used for estimators that support it,</span>
<span class="sd">          and `&#39;brute&#39;` is used otherwise. If `sample_weight` is not `None`,</span>
<span class="sd">          then `&#39;brute&#39;` is used regardless of the estimator.</span>

<span class="sd">        Please see :ref:`this note &lt;pdp_method_differences&gt;` for</span>
<span class="sd">        differences between the `&#39;brute&#39;` and `&#39;recursion&#39;` method.</span>

<span class="sd">    kind : {&#39;average&#39;, &#39;individual&#39;, &#39;both&#39;}, default=&#39;average&#39;</span>
<span class="sd">        Whether to return the partial dependence averaged across all the</span>
<span class="sd">        samples in the dataset or one value per sample or both.</span>
<span class="sd">        See Returns below.</span>

<span class="sd">        Note that the fast `method=&#39;recursion&#39;` option is only available for</span>
<span class="sd">        `kind=&#39;average&#39;` and `sample_weights=None`. Computing individual</span>
<span class="sd">        dependencies and doing weighted averages requires using the slower</span>
<span class="sd">        `method=&#39;brute&#39;`.</span>

<span class="sd">        .. versionadded:: 0.24</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predictions : :class:`~sklearn.utils.Bunch`</span>
<span class="sd">        Dictionary-like object, with the following attributes.</span>

<span class="sd">        individual : ndarray of shape (n_outputs, n_instances, \</span>
<span class="sd">                len(values[0]), len(values[1]), ...)</span>
<span class="sd">            The predictions for all the points in the grid for all</span>
<span class="sd">            samples in X. This is also known as Individual</span>
<span class="sd">            Conditional Expectation (ICE).</span>
<span class="sd">            Only available when `kind=&#39;individual&#39;` or `kind=&#39;both&#39;`.</span>

<span class="sd">        average : ndarray of shape (n_outputs, len(values[0]), \</span>
<span class="sd">                len(values[1]), ...)</span>
<span class="sd">            The predictions for all the points in the grid, averaged</span>
<span class="sd">            over all samples in X (or over the training data if</span>
<span class="sd">            `method` is &#39;recursion&#39;).</span>
<span class="sd">            Only available when `kind=&#39;average&#39;` or `kind=&#39;both&#39;`.</span>

<span class="sd">        grid_values : seq of 1d ndarrays</span>
<span class="sd">            The values with which the grid has been created. The generated</span>
<span class="sd">            grid is a cartesian product of the arrays in `grid_values` where</span>
<span class="sd">            `len(grid_values) == len(features)`. The size of each array</span>
<span class="sd">            `grid_values[j]` is either `grid_resolution`, or the number of</span>
<span class="sd">            unique values in `X[:, j]`, whichever is smaller.</span>

<span class="sd">            .. versionadded:: 1.3</span>

<span class="sd">        `n_outputs` corresponds to the number of classes in a multi-class</span>
<span class="sd">        setting, or to the number of tasks for multi-output regression.</span>
<span class="sd">        For classical regression and binary classification `n_outputs==1`.</span>
<span class="sd">        `n_values_feature_j` corresponds to the size `grid_values[j]`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    PartialDependenceDisplay.from_estimator : Plot Partial Dependence.</span>
<span class="sd">    PartialDependenceDisplay : Partial Dependence visualization.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; X = [[0, 0, 2], [1, 0, 0]]</span>
<span class="sd">    &gt;&gt;&gt; y = [0, 1]</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import GradientBoostingClassifier</span>
<span class="sd">    &gt;&gt;&gt; gb = GradientBoostingClassifier(random_state=0).fit(X, y)</span>
<span class="sd">    &gt;&gt;&gt; partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),</span>
<span class="sd">    ...                    grid_resolution=2) # doctest: +SKIP</span>
<span class="sd">    (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span>

   <span class="c1"># if not (is_classifier(estimator) or is_regressor(estimator)):</span>
   <span class="c1">#     raise ValueError(&quot;&#39;estimator&#39; must be a fitted regressor or classifier.&quot;)</span>

    <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Multiclass-multioutput estimators are not supported&quot;</span><span class="p">)</span>

    <span class="c1"># Use check_array only on lists and other non-array-likes / sparse. Do not</span>
    <span class="c1"># convert DataFrame into a NumPy array.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">&quot;__array__&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ensure_all_finite</span><span class="o">=</span><span class="s2">&quot;allow-nan&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_regressor</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span> <span class="ow">and</span> <span class="n">response_method</span> <span class="o">!=</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The response_method parameter is ignored for regressors and &quot;</span>
            <span class="s2">&quot;must be &#39;auto&#39;.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">kind</span> <span class="o">!=</span> <span class="s2">&quot;average&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;recursion&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The &#39;recursion&#39; method only applies when &#39;kind&#39; is set to &#39;average&#39;&quot;</span>
            <span class="p">)</span>
        <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;brute&quot;</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;recursion&quot;</span> <span class="ow">and</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The &#39;recursion&#39; method can only be applied when sample_weight is None.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;brute&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">BaseGradientBoosting</span><span class="p">)</span> <span class="ow">and</span> <span class="n">estimator</span><span class="o">.</span><span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;recursion&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">estimator</span><span class="p">,</span>
            <span class="p">(</span><span class="n">BaseHistGradientBoosting</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">),</span>
        <span class="p">):</span>
            <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;recursion&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;brute&quot;</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;recursion&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">estimator</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="n">BaseGradientBoosting</span><span class="p">,</span>
                <span class="n">BaseHistGradientBoosting</span><span class="p">,</span>
                <span class="n">DecisionTreeRegressor</span><span class="p">,</span>
                <span class="n">RandomForestRegressor</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">):</span>
            <span class="n">supported_classes_recursion</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;GradientBoostingClassifier&quot;</span><span class="p">,</span>
                <span class="s2">&quot;GradientBoostingRegressor&quot;</span><span class="p">,</span>
                <span class="s2">&quot;HistGradientBoostingClassifier&quot;</span><span class="p">,</span>
                <span class="s2">&quot;HistGradientBoostingRegressor&quot;</span><span class="p">,</span>
                <span class="s2">&quot;HistGradientBoostingRegressor&quot;</span><span class="p">,</span>
                <span class="s2">&quot;DecisionTreeRegressor&quot;</span><span class="p">,</span>
                <span class="s2">&quot;RandomForestRegressor&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Only the following estimators support the &#39;recursion&#39; &quot;</span>
                <span class="s2">&quot;method: </span><span class="si">{}</span><span class="s2">. Try using method=&#39;brute&#39;.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">supported_classes_recursion</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">response_method</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="n">response_method</span> <span class="o">=</span> <span class="s2">&quot;decision_function&quot;</span>

        <span class="k">if</span> <span class="n">response_method</span> <span class="o">!=</span> <span class="s2">&quot;decision_function&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;With the &#39;recursion&#39; method, the response_method must be &quot;</span>
                <span class="s2">&quot;&#39;decision_function&#39;. Got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">response_method</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_determine_key_type</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">accept_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;int&quot;</span><span class="p">:</span>
        <span class="c1"># _get_column_indices() supports negative indexing. Here, we limit</span>
        <span class="c1"># the indexing to be positive. The upper bound will be checked</span>
        <span class="c1"># by _get_column_indices()</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="mi">0</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;all features must be in [0, </span><span class="si">{}</span><span class="s2">]&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">features_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
        <span class="n">_get_column_indices</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">features</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intp</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span>
    <span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="n">feature_names</span> <span class="o">=</span> <span class="n">_check_feature_names</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span>

    <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">categorical_features</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">is_categorical</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">features_indices</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">categorical_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">categorical_features</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">categorical_features</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span>
            <span class="c1"># categorical features provided as a list of boolean</span>
            <span class="k">if</span> <span class="n">categorical_features</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">n_features</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When `categorical_features` is a boolean array-like, &quot;</span>
                    <span class="s2">&quot;the array should be of shape (n_features,). Got &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">categorical_features</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s2"> elements while `X` contains &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n_features</span><span class="si">}</span><span class="s2"> features.&quot;</span>
                <span class="p">)</span>
            <span class="n">is_categorical</span> <span class="o">=</span> <span class="p">[</span><span class="n">categorical_features</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">features_indices</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">categorical_features</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;U&quot;</span><span class="p">):</span>
            <span class="c1"># categorical features provided as a list of indices or feature names</span>
            <span class="n">categorical_features_idx</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">_get_feature_index</span><span class="p">(</span><span class="n">cat</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">cat</span> <span class="ow">in</span> <span class="n">categorical_features</span>
            <span class="p">]</span>
            <span class="n">is_categorical</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">idx</span> <span class="ow">in</span> <span class="n">categorical_features_idx</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">features_indices</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `categorical_features` to be an array-like of boolean,&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; integer, or string. Got </span><span class="si">{</span><span class="n">categorical_features</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
            <span class="p">)</span>

    <span class="n">grid</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">_grid_from_X</span><span class="p">(</span>
        <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">features_indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">percentiles</span><span class="p">,</span>
        <span class="n">is_categorical</span><span class="p">,</span>
        <span class="n">grid_resolution</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">response_method</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
        <span class="n">response_method</span> <span class="o">=</span> <span class="s2">&quot;predict&quot;</span>

    <span class="n">averaged_predictions</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">_partial_dependence_brute</span><span class="p">(</span>
        <span class="n">estimator</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">features_indices</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">response_method</span><span class="p">,</span> <span class="n">sample_weight</span>
    <span class="p">)</span>

    <span class="c1"># reshape predictions to</span>
    <span class="c1"># (n_outputs, n_instances, n_values_feature_0, n_values_feature_1, ...)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">*</span><span class="p">[</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">values</span><span class="p">]</span>
    <span class="p">)</span>


    <span class="c1"># reshape averaged_predictions to</span>
    <span class="c1"># (n_outputs, n_values_feature_0, n_values_feature_1, ...)</span>
    <span class="n">averaged_predictions</span> <span class="o">=</span> <span class="n">averaged_predictions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="p">[</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">values</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">pdp_results</span> <span class="o">=</span> <span class="n">Bunch</span><span class="p">(</span><span class="n">grid_values</span><span class="o">=</span><span class="n">values</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">kind</span> <span class="o">==</span> <span class="s2">&quot;average&quot;</span><span class="p">:</span>
        <span class="n">pdp_results</span><span class="p">[</span><span class="s2">&quot;average&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">averaged_predictions</span>
    <span class="k">elif</span> <span class="n">kind</span> <span class="o">==</span> <span class="s2">&quot;individual&quot;</span><span class="p">:</span>
        <span class="n">pdp_results</span><span class="p">[</span><span class="s2">&quot;individual&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">predictions</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># kind=&#39;both&#39;</span>
        <span class="n">pdp_results</span><span class="p">[</span><span class="s2">&quot;average&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">averaged_predictions</span>
        <span class="n">pdp_results</span><span class="p">[</span><span class="s2">&quot;individual&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">predictions</span>

    <span class="k">return</span> <span class="n">pdp_results</span></div>


</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">ordinal_xai</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../source/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/api/models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/api/interpretation.html">Interpretation Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/api/utils.html">Utilities</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
    </div>

    

    
  </body>
</html>