<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Utilities &#8212; Ordinal XAI 1.0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <script src="../_static/documentation_options.js?v=8d563738"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Interpretation Methods" href="interpretation.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="utilities">
<h1>Utilities<a class="headerlink" href="#utilities" title="Link to this heading">¶</a></h1>
<p>This section documents the utility functions and classes available in the package.</p>
<section id="evaluation-metrics">
<h2>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Link to this heading">¶</a></h2>
<dl class="py function" id="module-utils.evaluation_metrics">
<dt class="sig sig-object py" id="utils.evaluation_metrics.accuracy">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#accuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.accuracy" title="Link to this definition">¶</a></dt>
<dd><p>Calculate accuracy for ordinal regression.</p>
<section id="parameters">
<h3>Parameters:<a class="headerlink" href="#parameters" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
</dl>
</section>
<section id="returns">
<h3>Returns:<a class="headerlink" href="#returns" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Accuracy score</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.adjacent_accuracy">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">adjacent_accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#adjacent_accuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.adjacent_accuracy" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Adjacent Accuracy for ordinal regression.
This measures the proportion of predictions that are either correct or off by one class.</p>
<section id="id1">
<h3>Parameters:<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
</dl>
</section>
<section id="id2">
<h3>Returns:<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Adjacent Accuracy score between 0 and 1</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.cem">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">cem</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#cem"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.cem" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Closeness Evaluation Measure (CEM) for ordinal classification.</p>
<section id="id3">
<h3>Parameters:<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels (gold standard)</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels (system predictions)</p>
</dd>
</dl>
</section>
<section id="id4">
<h3>Returns:<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>CEM score between 0 and 1, where lower values indicate better performance</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.evaluate_ordinal_model">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">evaluate_ordinal_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred_proba</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#evaluate_ordinal_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.evaluate_ordinal_model" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate an ordinal regression model using multiple metrics.</p>
<section id="id5">
<h3>Parameters:<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
<dt>y_pred_proba<span class="classifier">array-like of shape (n_samples, n_classes), optional</span></dt><dd><p>Predicted probabilities for each class</p>
</dd>
</dl>
</section>
<section id="id6">
<h3>Returns:<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>dict</dt><dd><p>Dictionary containing all evaluation metrics</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.kendall_tau">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">kendall_tau</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#kendall_tau"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.kendall_tau" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Kendall’s Tau correlation coefficient for ordinal data.
This measures the ordinal association between two rankings.</p>
<section id="id7">
<h3>Parameters:<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
</dl>
</section>
<section id="id8">
<h3>Returns:<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Kendall’s Tau correlation coefficient between -1 and 1
where 1 indicates perfect agreement and -1 indicates perfect disagreement</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.mae">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">mae</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#mae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.mae" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Mean Absolute Error (MAE) for ordinal regression.</p>
<section id="id9">
<h3>Parameters:<a class="headerlink" href="#id9" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
</dl>
</section>
<section id="id10">
<h3>Returns:<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Mean Absolute Error</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.mse">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">mse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#mse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.mse" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Mean Squared Error (MSE) for ordinal regression.</p>
<section id="id11">
<h3>Parameters:<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
</dl>
</section>
<section id="id12">
<h3>Returns:<a class="headerlink" href="#id12" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Mean Squared Error</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.mze">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">mze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#mze"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.mze" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Mean Zero-One Error (MZE) for ordinal regression.
MZE = 1 - accuracy</p>
<section id="id13">
<h3>Parameters:<a class="headerlink" href="#id13" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
</dl>
</section>
<section id="id14">
<h3>Returns:<a class="headerlink" href="#id14" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Mean Zero-One Error</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.ordinal_classification_index">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">ordinal_classification_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#ordinal_classification_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.ordinal_classification_index" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Ordinal Classification Index (OCI) for ordinal regression.
OCI = 1 - (sum(<a href="#id28"><span class="problematic" id="id29">|y_true - y_pred|</span></a>) / (n * (max_class - min_class)))</p>
<section id="id15">
<h3>Parameters:<a class="headerlink" href="#id15" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
</dl>
</section>
<section id="id16">
<h3>Returns:<a class="headerlink" href="#id16" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Ordinal Classification Index</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.ordinal_log_loss">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">ordinal_log_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred_proba</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-15</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#ordinal_log_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.ordinal_log_loss" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Ordinal Log Loss for ordinal regression.
OLL = -sum(y_true_ij * log(y_pred_ij))</p>
<section id="id17">
<h3>Parameters:<a class="headerlink" href="#id17" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred_proba<span class="classifier">array-like of shape (n_samples, n_classes)</span></dt><dd><p>Predicted probabilities for each class</p>
</dd>
<dt>eps<span class="classifier">float, optional</span></dt><dd><p>Small value to avoid log(0)</p>
</dd>
</dl>
</section>
<section id="id18">
<h3>Returns:<a class="headerlink" href="#id18" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Ordinal Log Loss</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.ordinal_weighted_ce">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">ordinal_weighted_ce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred_proba</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#ordinal_weighted_ce"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.ordinal_weighted_ce" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Ordinal Weighted Cross-Entropy (OWCE) for ordinal regression.
OWCE = sum(w_ij * log(y_pred_ij)) where w_ij is the normalized weight
based on the distance between classes.</p>
<section id="id19">
<h3>Parameters:<a class="headerlink" href="#id19" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred_proba<span class="classifier">array-like of shape (n_samples, n_classes)</span></dt><dd><p>Predicted probabilities for each class</p>
</dd>
</dl>
</section>
<section id="id20">
<h3>Returns:<a class="headerlink" href="#id20" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Ordinal Weighted Cross-Entropy</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.print_evaluation_results">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">print_evaluation_results</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#print_evaluation_results"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.print_evaluation_results" title="Link to this definition">¶</a></dt>
<dd><p>Print evaluation results in a formatted way.</p>
<section id="id21">
<h3>Parameters:<a class="headerlink" href="#id21" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>results<span class="classifier">dict</span></dt><dd><p>Dictionary containing evaluation metrics</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.ranked_probability_score">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">ranked_probability_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred_proba</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#ranked_probability_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.ranked_probability_score" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Ranked Probability Score (RPS) for ordinal regression.
RPS = sum((F_i - O_i)^2) where F_i is the cumulative predicted probability
and O_i is the cumulative observed probability.</p>
<section id="id22">
<h3>Parameters:<a class="headerlink" href="#id22" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred_proba<span class="classifier">array-like of shape (n_samples, n_classes)</span></dt><dd><p>Predicted probabilities for each class</p>
</dd>
</dl>
</section>
<section id="id23">
<h3>Returns:<a class="headerlink" href="#id23" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Ranked Probability Score</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.spearman_correlation">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">spearman_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#spearman_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.spearman_correlation" title="Link to this definition">¶</a></dt>
<dd><p>Calculate Spearman rank correlation for ordinal regression.</p>
<section id="id24">
<h3>Parameters:<a class="headerlink" href="#id24" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
</dl>
</section>
<section id="id25">
<h3>Returns:<a class="headerlink" href="#id25" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Spearman rank correlation coefficient</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.evaluation_metrics.weighted_kappa">
<span class="sig-prename descclassname"><span class="pre">utils.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">weighted_kappa</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'quadratic'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/utils/evaluation_metrics.html#weighted_kappa"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#utils.evaluation_metrics.weighted_kappa" title="Link to this definition">¶</a></dt>
<dd><p>Calculate weighted kappa for ordinal regression.</p>
<section id="id26">
<h3>Parameters:<a class="headerlink" href="#id26" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>y_true<span class="classifier">array-like</span></dt><dd><p>True ordinal labels</p>
</dd>
<dt>y_pred<span class="classifier">array-like</span></dt><dd><p>Predicted ordinal labels</p>
</dd>
<dt>weights<span class="classifier">str, optional</span></dt><dd><p>Weighting scheme for the confusion matrix. Options: ‘linear’, ‘quadratic’, ‘none’</p>
</dd>
</dl>
</section>
<section id="id27">
<h3>Returns:<a class="headerlink" href="#id27" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>float</dt><dd><p>Weighted kappa score</p>
</dd>
</dl>
</section>
</dd></dl>

<p>The evaluation_metrics module provides various metrics for evaluating ordinal regression models, including:</p>
<ul class="simple">
<li><p>Accuracy</p></li>
<li><p>Mean Absolute Error (MAE)</p></li>
<li><p>Mean Squared Error (MSE)</p></li>
<li><p>Weighted Kappa</p></li>
<li><p>Ordinal Weighted Cross-Entropy</p></li>
<li><p>Ranked Probability Score</p></li>
</ul>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Ordinal XAI</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="interpretation.html">Interpretation Methods</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Utilities</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-metrics">Evaluation Metrics</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="interpretation.html" title="previous chapter">Interpretation Methods</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Jakob Wankmüller.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/api/utils.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>